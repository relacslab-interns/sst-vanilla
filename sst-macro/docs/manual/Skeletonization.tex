% !TEX root = manual.tex

\chapter{External Applications and Skeletonization}
\label{chap:appsAndSkeletonization}

\section{Basic Application porting}
\label{sec:skel:basic}

There are three parts to successfully taking a C++ code and turning it into a scalable simulation.
\begin{itemize}
\item Symbol Interception: Rather than linking to MPI, pThreads, or other parallel libraries (or even calling \inlinecode{hostname}), these functions must be redirected to \sstmacro rather than calling the native libraries on the machine running the simulator.
You get all redirected linkage for free by using
the SST compiler wrappers \inlineshell{sst++} and \inlineshell{sstcc} installed in the \inlineshell{bin} folder.
\item Skeletonization: While \sstmacro can run in emulation mode, executing your entire application exactly, this is not scalable.  To simulate at scale (i.e. 1K or more MPI ranks) you must strip down or ``skeletonize'' the application to the minimal amount of computation.  The energy and time cost of expensive compute kernels are then simulated via models rather than explicitly executed. 
\item Process encapsulation: Each virtual process being simulated is not an actual physical process. It is instead modeled as a lightweight user-space thread.  This means each virtual process has its own stack and register variables, but not its own data segment (global variables).
Virtual processes share the same address space and the same global variables.  A Beta version of the auto-skeletonizing clang-based SST compiler is available with the 7.X releases. If the Beta is not stable with your application, manual refactoring may be necessary if you have global variables.
\end{itemize}

Now in Beta, another possible feature is available:
\begin{itemize}
\item Memoization hooks: Rather than building an app for simulation, build an app with special profiling hooks. There is no skeletonization or redirected linkage, but the runtime or performance counters obtained by running should be used to build models for simulation.
\end{itemize}

There are generally 3 modes of using an application common with SST/macro:
\begin{itemize}
\item Simulation: As lightweight as possible without sacrificing accuracy. Intended to be used for performance estimation at large scales.
\item Emulation/Virtualization: Run a parallel or distributed application within a single simulator thread/process - primarily useful for debugging.
\item Memoization: Run a full application within the simulator, collecting performance counters or timers on critical sections
\end{itemize}

\begin{center}
\begin{tabular}{l l l l l}
\hline
 & Simulator hooks & Symbol interception & Skeletonization & Process encapsulation \\
\hline
\hline
Simulation & Yes & Yes & Yes & Yes \\
Virtualization & Yes & Yes & No & Yes \\
Memoization & Yes & No & No & No \\
\hline
\end{tabular}
\end{center}

\subsection{Loading external skeletons with the standalone core}
\label{subsec:externalAppStandalone}

You should always write your own skeleton applications in an external folder rather then integrating directly into the \inlineshell{sstmac} executable.
Existing make systems can be used unmodified. Rather than producing an executable, though, SST/macro produces a shared library.
These shared libraries are then imported into the main \inlinecode{sstmac} executable.

If you follow the example in the \inlineshell{skeletons/sendrecv} folder,
the Makefile shows how to generate an importable skeleton.
If you are using \inlineshell{sst++}, it will automatically convert executables into loadable libraries.
If your application is named \inlineshell{runapp}, you would run it with \inlineshell{sstmac}:

\begin{ShellCmd}
./runapp -f parameters.ini --exe=./runapp
\end{ShellCmd}
directing to load the library as a skeleton executable.

\section{Auto-skeletonization with Clang}
\label{sec:autoSkeletonization}

The build of the Clang toolchain is described in Section \ref{sec:buildingClang}. 
This enables a source-to-source translation capability in the \inlineshell{sst++} compiler that can auto-skeletonize computation and fix global variable references.
Some of this can be accomplished automatically (global variables), but most of it (removing computation and memory allocations) must occur through pragmas.
A good example of skeletonization can be found in the lulesh2.0.3 example in the skeletons folder. Most of the available SST pragmas are used there.
Pragmas are preferred since they allow switching easily back and forth between skeleton and full applications.
This allows much easier validation of the simulation. The section here briefly introduces the SST pragma language.
A complete tutorial on all available pragmas is given in Chapter \ref{clangTutorial}.

\subsection{Redirecting Main}
\label{subsec:redirectMain}

Your application's \inlinecode{main} has to have its symbols changed.
The simulator itself takes over \inlinecode{main}.
\sstmacro therefore has to capture the function pointer in your code and associate it with a string name for the input file.
This is automatically accomplished by defining the macro \inlinecode{sstmac_app_name} either in your code or through a \inlineshell{-D=} build flag to the name of your application (unquoted!). The value of the macro will become the string name used for launching the application via \inlinecode{node.app1.name=X}.
Even without Clang, this works for C++. For C, Clang source-to-source is required.

\subsection{Memory Allocations}
\label{subsec:memoryAllocations}

To deactivate memory allocations in C code that uses \inlinecode{malloc}, use:
\begin{CppCode}
#pragma sst malloc
  void* ptr = malloc(...)
\end{CppCode}
prior to any memory allocations that should be deactivated during skeleton runs, but active during real runs.

Similarly, for C++ we have
\begin{CppCode}
#pragma sst new
  int* ptr = new int[...]
\end{CppCode}

\subsection{Computation}
\label{subsec:computation}

In general, the SST compiler captures all \inlinecode{#pragma omp parallel} statements.
It then analyzes the for-loop or code block and attempts to derive a computational model for it.
The computational models are quite simple (skeleton apps!), 
based simply on the number of flops executed and the number of bytes read (written) from memory.
Consider the example:

\begin{CppCode}
double A[N], B[N];
#pragma omp parallel for
for (int i=0; i < N; ++i){
  A[i] = alpha*A[i] + B[i];
}
\end{CppCode}
The SST compiler deduces $16N$ bytes read, $8N$ bytes written, and $16N$ flops (or $8N$ if fused-multiplies are enabled).
Based on processor speed and memory speed, it then estimates how long the kernel will take without actually executing the loop.
If not wanting to use OpenMP in the code, \inlinecode{#pragma sst compute} can be used instead of \inlinecode{#pragma omp parallel}.

\subsection{Special Pragmas}
\label{subsec:specialPragams}

Many special cases can arise that break skeletonization.
This is often not a limit of the SST compiler, but rather a fundemental limitation in the static analysis of the code.
This most often arises due to nested loops. Consider the example:

\begin{CppCode}
#pragma omp parallel for
for (int i=0; i < N; ++i){
  int nElems = nElemLookup[i];
  for (int e=0; e < nElems; ++e){
  }
}
\end{CppCode}
Auto-skeletonization will fail. The skeletonization converts the outer loop into a single call to an SST compute model.
However, the inner loop can vary depending on the index.
This data-dependency breaks the static analysis.
To fix this, a hint must be given to SST as to what the ``average'' inner loop size is.
For example, it may loops nodes in a mesh. In this case, it may almost always be 8.

\begin{CppCode}
#pragma omp parallel for
for (int i=0; i < N; ++i){
  int nElems = nElemLookup[i];
  #sst replace nElems 8
  for (int e=0; e < nElems; ++e){
  }
}
\end{CppCode}
This hint allows SST to skeletonize the inner loop and ``guess'' at the data dependency.



\subsection{Skeletonization Issues}
\label{subsec:skeletonIssues}

Skeletonization challenges fall into three main categories:

\begin{itemize}
\item \textit{Data structures} - Memory is a precious commodity when running large simulations, so get rid of every memory allocation you can.
\item \textit{Loops} - Usually the main brunt of CPU time, so get rid of any loops that don't contain MPI calls or calculate variables needed in MPI calls.
\item \textit{Communication buffers} - While you can pass in real buffers with data to \sstmacro MPI calls and they will work like normal, it is relatively expensive. If they're not needed, get rid of them.
\end{itemize}





The main issue that arises during skeletonization is data-dependent communication.  
In many cases, it will seem like you can't remove computation or memory allocation because MPI calls depend somehow on that data.  
The following are some examples of how we deal with those:

\begin{itemize}
\item \textit{Loop convergence} - In some algorithms, the number of times you iterate through the main loop depends on an error converging to near zero, or some other converging mechanism.  This basically means you can't take out anything at all, because the final result of the computation dictates the number of loops.  In this case, we usually set the number of main loop iterations to a fixed number.  
\item \textit{Particle migration} - Some codes have a particle-in-cell structure, where the spatial domain is decomposed among processes, and particles or elements are distributed among them, and forces between particles are calculated.  When a particle moves to another domain/process, how many particles migrate and how far depends on the actual computed forces. However, in the skeleton, we are not actually computing the forces - only estimated how long the force computation took.  If all we need to know is that this migration/communication happens sometimes, then we can just make it happen every so many iterations, or even sample from a probability distribution.  
\item \textit{AMR} - Some applications, like adaptive mesh refinement (AMR), exhibit communication that is entirely dependent on the computation.  In this case, skeletonization again depends on making approximations or probability estimates of where and when box refinement occurs without actually computing everything.
\end{itemize}

For applications with heavy dynamic data dependence, we have the following strategies:
\begin{itemize}
\item \textit{Traces}  - revert to DUMPI traces, where you will be limited by existing machine size.  Trace extrapolation is also an option here.
\item \textit{Synthetic} - It may be possible to replace communication with randomly-generated data and decisions, which emulate how the original application worked. This occurs in the CoMD skeleton.
\item \textit{Hybrid} - It is possible to construct meta-traces that describe the problem from a real run, and read them into \sstmacro to reconstruct the communication that happens.  This occurs in the \inlineshell{boxml} aplications.
\end{itemize}

\section{Process Encapsulation}
\label{sec:processEncapsulation}

As mentioned above, virtual processes are not real, physical processes inside the OS.
They are explicitly managed user-space threads with a private stack, but without a private set of global variables.
When porting an application to SST/macro, global variables used in C programs will not be mapped to separate memory addresses causing incorrect execution or even segmentation faults.
If you have avoided global variables, there is no major issue.  
If you have read-only global variables with the same value on each machine, there is still no issue.
If you have mutable global variables, you should use the \inlineshell{sst++} clang-based compiler wrappers to auto-refactor your code (Section \ref{sec:autoSkeletonization}).
This feature is current labeled Beta, but is stable for numerous tests and will be fully supported for release 7.1.

